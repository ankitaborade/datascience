# -*- coding: utf-8 -*-
"""mini project 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15wuF34tFHITyT0JO0fc7-kqt_-S749wS
"""

# Commented out IPython magic to ensure Python compatibility.
#importing all the required python libraries 
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

df = pd.read_csv('h.csv')# reading the csv file 
df.head()# reading the dataset first five rows

df.columns  # getting the columnns of dataset

df.isnull().sum()  # checking the null values in te dataset

df.describe().T # checking the statistical inforamtion about the dataset

plt.figure(figsize = (20, 15)) # setting the figure size 
sns.heatmap(df.corr(), cmap = "CMRmap", annot= True) # finding the correlation using the heatmap

df.cov()  #finding the covarience

df.info() #checking the null values and datatypes of each columns in dataset

### Checking the distribution of the features
plt.figure(figsize=(20,40),facecolor='white')  #setting the figure size 
pltnumber = 1  #initailizing the counter 
for column in df:
  if pltnumber<=14: ## Since there are 14 columns
    ax = plt.subplot(7,2,pltnumber) 
    sns.distplot(df[column]) # ploting the dislot for each plot 
    plt.xlabel(column,fontsize=20) # labeling each plot with column name 
  pltnumber+=1  
plt.show()

# Function to detect outliers
def outlier_thresholds(dataframe,variable):  
  quartile1=dataframe[variable].quantile(0.25) 
  quartile3=dataframe[variable].quantile(0.75)
  interquartile_range=quartile3-quartile1
  up_limit=quartile3+1.5*interquartile_range
  low_limit=quartile1-1.5*interquartile_range
  return low_limit,up_limit

for i in df.columns:  #printing the limits of every column 
  print("Lower Limit and Up Limit of {} is {}".format(i,outlier_thresholds(df,i)))

plt.figure(figsize = (20,40)) #setting the figure size 
pltnumber=1

#ploting the subplot for every column using for loop
for column in df:
  if pltnumber<=14:
    ax = plt.subplot(7,2,pltnumber)
    sns.boxplot(df[column])
    plt.xlabel(column, fontsize = 20)
  pltnumber += 1
plt.show()

#replacing the outliers with threshold values 
def replace_with_thresholds(dataframe, column):
  for variable in column:
    low_limit, up_lmt = outlier_thresholds(dataframe, variable)
    dataframe.loc[(dataframe[variable]< low_limit), variable] = low_limit
    dataframe.loc[(dataframe[variable]> up_limit), variable] = up_limit
  replace_with_thresholds(df, df.ccolumns)

plt.figure(figsize = (20,40)) #assiging the figure size 
pltnumber=1
#ploting the subplot for every coloumn 
for column in df:
  if pltnumber<=14:
    ax = plt.subplot(7,2,pltnumber)
    sns.boxplot(df[column])
    plt.xlabel(column, fontsize = 20)
  pltnumber += 1
plt.show()

df1 = df.copy() #making the copy of dataset 
#trying to remove outliers with IQR
def remove_outlier_IQR(col):
  percentile25 = df1[col].quantile(.25)
  percentile75 = df1[col].quantile(.75)
  print("percentile25", percentile25)
  print("percentile75", percentile75)
  iqr = percentile75 - percentile25 
  upper_limit = percentile75 + 1.5*iqr
  lower_limit = percentile25 - 1.5*iqr
  print("Upper limit", upper_limit)
  print("Lower Limit", lower_limit)
  df1[col] = np.where(df1[col]> upper_limit, upper_limit, np.where(df1[col]<lower_limit, lower_limit, df1[col]))
  return df1[df1[col]>upper_limit]

remove_outlier_IQR('trestbps') #passing the trestbps column to IQR for finding the upper limit and lower limit for removing the outliers

remove_outlier_IQR('chol')  #passing the chol column to IQR for finding the upper limit and lower limit for removing the outliers

remove_outlier_IQR('fbs')  #passing the fbs column to IQR for finding the upper limit and lower limit for removing the outliers

remove_outlier_IQR('thalach')  #passing the thalach column to IQR for finding the upper limit and lower limit for removing the outliers

remove_outlier_IQR('oldpeak')  #passing the oldpeak column to IQR for finding the upper limit and lower limit for removing the outliers

remove_outlier_IQR('ca')  #passing the ca column to IQR for finding the upper limit and lower limit for removing the outliers

remove_outlier_IQR('thal')  #passing the thal column to IQR for finding the upper limit and lower limit for removing the outliers

#comparing the both dataset to see the outliers are removed or not 
def creat_comparsion_plot(df, df1, column):
  plt.figure(figsize = (16,8))
  plt.subplot(2,2,1)
  sns.distplot(df[column])

  plt.subplot(2,2,2)
  sns.boxplot(df[column])

  plt.subplot(2,2,3)
  sns.distplot(df1[column])

  plt.subplot(2,2,4)
  sns.boxplot(df1[column])

  plt.show()


creat_comparsion_plot(df, df1, 'trestbps')

creat_comparsion_plot(df, df1, 'chol')  #passing the chol column to create_comparison_plot for cheching the outliers present or not

creat_comparsion_plot(df, df1, 'fbs')  #passing the fbs column to create_comparison_plot for cheching the outliers present or not

creat_comparsion_plot(df, df1, 'thalach')  #passing the thalach column to create_comparison_plot for cheching the outliers present or not

creat_comparsion_plot(df, df1, 'oldpeak')   #passing the oldpeak column to create_comparison_plot for cheching the outliers present or not

creat_comparsion_plot(df, df1, 'ca')  #passing the ca column to create_comparison_plot for cheching the outliers present or not

creat_comparsion_plot(df, df1, 'thal')  #passing the thal column to create_comparison_plot for cheching the outliers present or not

df1.skew()  #finding the skewnness for fd1 dataset

x = df1.drop(columns = ['target'])  #droping the target ccolumn from df1 dataset
y = df1['target']  #assiging the target to y variable as dependent variable

#Relationship between dependent and independent features
plt.figure(figsize=(20,25), facecolor = 'White')
plotnumber = 1

for column in x:
  if plotnumber < 14:
    ax = plt.subplot(7,2,plotnumber)
    sns.stripplot(y,x[column])
  plotnumber += 1
plt.tight_layout()

#Graphical Analysis between dependent and independent features
plt.figure(figsize= (20,40))

for i in enumerate(x.columns):
  plt.subplot(7,2, i[0]+1)
  sns.regplot(x=df1[i[1]],y=df1['target'])

#importing the train_test_split function 
from sklearn.model_selection import train_test_split
#spliting the dataset into training and test data
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = .15, random_state = 16)

x_train  #printing the x_train data

x_test.shape  #seeing the number of columns for x_test

y_train.shape  #seeing the number of columns for y_train

y_test.shape  #seeing the number of columns for y_test

#importing the StandardScaler function 
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)  #Scaling the data
x_train

x_test = scaler.transform(x_test)
x_test

# importing logistic regression from sklearn library
from sklearn.linear_model import LogisticRegression

classifier = LogisticRegression()# Assigning the classifier to LogisticRegression
classifier.fit(x_train, y_train)   # fitting the trianing data
LogisticRegression()

y_pred = classifier.predict(x_test)  # Storing the predicted data from test to y_pred 
y_pred

from sklearn.metrics import accuracy_score,classification_report   #importing the accuracy_score and classification_report 
score = accuracy_score(y_pred, y_test)  #Calculating the accuracy for logistic regression 
print(score)  #print the accuracy for logistic regression

print(classification_report(y_pred, y_test))  # creating the classification report for Logistics Regression

# importing SVM from sklearn library
from sklearn.svm import SVC
classifier = SVC(kernel = 'linear', random_state = 0)  # Assigning the classifier to SVM
classifier.fit(x_train, y_train)  # fitting the trianing data
SVC(kernel = 'linear', random_state = 0)

y_pred = classifier.predict(x_test)  # Storing the predicted data from test to y_pred 
y_pred

score = accuracy_score(y_pred, y_test)  #Calculating the accuracy for SVM 
score #print the accuracy for SVM

print(classification_report(y_pred, y_test))  # creating the classification report for SVM

# importing Decision Tree from sklearn library
from sklearn.tree import DecisionTreeClassifier

treemodel = DecisionTreeClassifier()  # Assigning the classifier to Decision Tree
treemodel.fit(x_train, y_train)  # fitting the trianing data
DecisionTreeClassifier()

# importing tree from sklearn library
from sklearn import tree
plt.figure(figsize = (20,17))
tree.plot_tree(treemodel, filled=True)

y_pred = treemodel.predict(x_test)  # Storing the predicted data from test to y_pred 
y_pred

score = accuracy_score(y_pred, y_test)  #Calculating the accuracy for Decission Tree 
score  #print the accuracy

print(classification_report(y_pred,y_test))  # creating the classification report for Decission tree

# importing Random Forest Classifier from sklearn library
from sklearn.ensemble import RandomForestClassifier

classifier = RandomForestClassifier(n_estimators = 50)  # Assigning the classifier to Random forest

classifier.fit(x_train, y_train)  # fitting the trianing data
RandomForestClassifier(n_estimators = 50)

y_pred = classifier.predict(x_test)  # Storing the predicted data from test to y_pred 
y_pred

score = accuracy_score(y_pred, y_test) #Calculating the accuracy for Random Forest

score  #print the accuracy

print(classification_report(y_pred, y_test))  # creating the classification report for Random forest

# importing Bagging Classifier from sklearn library
from sklearn.ensemble import BaggingClassifier
BC = BaggingClassifier(base_estimator = None, n_estimators = 10, max_samples=1.0, max_features =1.0, bootstrap_features = False,
                  oob_score = False)  # Assigning the classifier to Bagging classifier

BC.fit(x_train,y_train)  # fitting the trianing data
BaggingClassifier()

y_pred = BC.predict(x_test)  # Storing the predicted data from test to y_pred  
y_pred

score = accuracy_score(y_pred, y_test) #Calculating the accuracy for Bagging Classifier
score  #print the accuracy

print(classification_report(y_pred, y_test))  # creating the classification report for Bagging

#importing the Adaboost classifier 
from sklearn.ensemble import AdaBoostClassifier 
classifier = AdaBoostClassifier(n_estimators =100, random_state = 0)  # Assigning the classifier to AdaBoost

classifier.fit(x_train, y_train)  # fitting the trianing data
AdaBoostClassifier(n_estimators=100, random_state = 0)

y_pred = classifier.predict(x_test)  # Storing the predicted data from test to y_pred 
y_pred

score = accuracy_score(y_pred, y_test) #Calculating the accuracy for  Adaboost
score  #print the accuracy

print(classification_report(y_pred, y_test))  # creating the classification report for Adaboost

from sklearn.ensemble import GradientBoostingClassifier   #importing the Gradient Boost Classifier 
classifier = GradientBoostingClassifier(n_estimators = 100, learning_rate = 1., max_depth = 1, random_state = 0)  # Assigning the classifier to GradienntBoost

classifier.fit(x_train, y_train)  # fitting the trianing data
GradientBoostingClassifier(n_estimators= 100, learning_rate = 1.0, max_depth = 1, random_state = 0)

y_pred = classifier.predict(x_test) # Storing the predicted data from test to y_pred 
y_pred

score = accuracy_score(y_pred, y_test)  #Calculating the accuracy for Gradient Boost
score  #print the accuracy

print(classification_report(y_pred, y_test))  # creating the classification report for Gradient Boost

#importing the XGBoost classifier 
from xgboost import XGBClassifier
model = XGBClassifier()  # Assigning the classifier to XGBoost

model.fit(x_train, y_train)  # fitting the trianing data

y_pred = model.predict(x_test)  # Storing the predicted data from test to y_pred 

y_pred

score = accuracy_score(y_pred, y_test)  #Calculating the accuracy for XGBoost 
score  #print the accuracy

print(classification_report(y_pred, y_test))  # creating the classification report for XGBoost